{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definire la UNET\n",
    "\n",
    "def unet(pretrained_weights=None, input_size=(256, 256, 1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "    encoder=Model(inputs=inputs, outputs=drop5)\n",
    "    # model.summary()\n",
    "\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_path = '../dataset/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\n",
    "\n",
    "df_train = pd.read_csv(train_txt_path, sep=\" \", header=None)\n",
    "df_train.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_train = df_train.drop(columns=\"null\")\n",
    "\n",
    "dev_txt_path = '../dataset/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt'\n",
    "\n",
    "df_dev = pd.read_csv(dev_txt_path, sep=\" \", header=None)\n",
    "df_dev.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_dev = df_dev.drop(columns=\"null\")\n",
    "\n",
    "eval_txt_path = '../dataset/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'\n",
    "\n",
    "df_eval = pd.read_csv(eval_txt_path, sep=\" \", header=None)\n",
    "df_eval.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_eval = df_eval.drop(columns=\"null\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i parametri da testare sono:\n",
    "# nfft = 64, hop_size = 32 fatto\n",
    "# nfft = 128, hop_size = 64 fatto\n",
    "# nfft = 256, hop_size = 128 fatto\n",
    "# nffr = 512, hop_size = 256\n",
    "\n",
    "nfft = 128\n",
    "hop_size = 64 \n",
    "\n",
    "alg_list = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06']\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16 #=63 per nfft < 512\n",
    "\n",
    "input_size = nfft // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800/3800 [00:29<00:00, 131.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2850 samples, validate on 950 samples\n",
      "Epoch 1/100\n",
      "2850/2850 [==============================] - 85s 30ms/sample - loss: 0.1285 - accuracy: 0.0000e+00 - val_loss: 0.1194 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2850/2850 [==============================] - 73s 26ms/sample - loss: 0.1257 - accuracy: 0.0000e+00 - val_loss: 0.1168 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2850/2850 [==============================] - 74s 26ms/sample - loss: 0.1230 - accuracy: 0.0000e+00 - val_loss: 0.1142 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2848/2850 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "\n",
    "for alg in alg_list:\n",
    "    alg_df_train = df_train[df_train['system_id']==alg]\n",
    "    \n",
    "    unet_input = []\n",
    "    \n",
    "    for index, row in tqdm.tqdm(alg_df_train.iterrows(), total=alg_df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # qui puoi concatenare le magnitude per l'input alla rete nella variabile unet_input\n",
    "        unet_input.append(mag)\n",
    "        \n",
    "    unet_input = np.array(unet_input)\n",
    "    unet_input = unet_input[..., np.newaxis]\n",
    "    \n",
    "    train_validation_index = int(np.round(unet_input.shape[0] * 0.75))\n",
    "    X_train = unet_input[:train_validation_index]\n",
    "    X_valid = unet_input[train_validation_index:]\n",
    "\n",
    "    model_checkpoint_name = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}.ckpt'.format(\n",
    "        nfft, hop_size, alg)\n",
    "    \n",
    "    history_name = '../features/unet/history/train_nfft_{}_hop_size_{}_alg_{}.npy'.format(\n",
    "        nfft, hop_size, alg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model, encoder = unet(None, (input_size, input_size, 1))\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_checkpoint_name, \n",
    "                                 monitor='val_loss', \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=True, \n",
    "                                 mode='auto',\n",
    "                                 verbose=0)\n",
    "    \n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_valid, X_valid),\n",
    "                    callbacks=[ checkpoint, early])\n",
    "    np.save(history_name, history.history)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "for alg in alg_list:\n",
    "    \n",
    "    # carichiamo il modello corrispondente dalla cartella \n",
    "    model_folder = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "\n",
    "    # creiamo un campo in un dataframe per ora vuoto\n",
    "    feat_name = 'unet_mse_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "    df_train[feat_name] = np.nan\n",
    "    \n",
    "    break\n",
    "    for index, row in tqdm.tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # calcoliamo l'MSE \n",
    "        \n",
    "        # salviamo l'MSE nel corrispondente campo\n",
    "        \n",
    "        # df_train.at[index, feat_name] = mse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev_feat_root_path = '../features/bicoherences/dev_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "for alg in alg_list:\n",
    "    \n",
    "    # carichiamo il modello corrispondente dalla cartella \n",
    "    model_folder = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "\n",
    "    # creiamo un campo in un dataframe per ora vuoto\n",
    "    feat_name = 'unet_mse_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "    df_train[feat_name] = np.nan\n",
    "    \n",
    "    break\n",
    "    for index, row in tqdm.tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # calcoliamo l'MSE \n",
    "        \n",
    "        # salviamo l'MSE nel corrispondente campo\n",
    "        \n",
    "        # df_train.at[index, feat_name] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "for alg in alg_list:\n",
    "    \n",
    "    # carichiamo il modello corrispondente dalla cartella \n",
    "    model_folder = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "\n",
    "    # creiamo un campo in un dataframe per ora vuoto\n",
    "    feat_name = 'unet_mse_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "    df_train[feat_name] = np.nan\n",
    "    \n",
    "    break\n",
    "    for index, row in tqdm.tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # calcoliamo l'MSE \n",
    "        \n",
    "        # salviamo l'MSE nel corrispondente campo\n",
    "        \n",
    "        # df_train.at[index, feat_name] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
