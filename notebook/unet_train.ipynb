{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definire la UNET\n",
    "\n",
    "def unet(pretrained_weights=None, input_size=(256, 256, 1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "    encoder=Model(inputs=inputs, outputs=drop5)\n",
    "    # model.summary()\n",
    "\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_path = '../dataset/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\n",
    "\n",
    "df_train = pd.read_csv(train_txt_path, sep=\" \", header=None)\n",
    "df_train.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_train = df_train.drop(columns=\"null\")\n",
    "\n",
    "dev_txt_path = '../dataset/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt'\n",
    "\n",
    "df_dev = pd.read_csv(dev_txt_path, sep=\" \", header=None)\n",
    "df_dev.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_dev = df_dev.drop(columns=\"null\")\n",
    "\n",
    "eval_txt_path = '../dataset/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'\n",
    "\n",
    "df_eval = pd.read_csv(eval_txt_path, sep=\" \", header=None)\n",
    "df_eval.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_eval = df_eval.drop(columns=\"null\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i parametri da testare sono:\n",
    "# nfft = 64, hop_size = 32 \n",
    "# nfft = 128, hop_size = 64 \n",
    "# nfft = 256, hop_size = 128 \n",
    "# nffr = 512, hop_size = 256\n",
    "\n",
    "nfft = 128\n",
    "hop_size = 64 \n",
    "\n",
    "alg_list = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06']\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16 #=63 per nfft < 512\n",
    "\n",
    "input_size = nfft // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train UNET normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 2498/3800 [00:02<00:01, 973.20it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2f9c41435dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg_df_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malg_df_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfeat_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feat_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'audio_filename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbicoh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbicoh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbicoh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/home/cborrelli/miniconda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/home/cborrelli/miniconda/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "\n",
    "for alg in alg_list:\n",
    "    alg_df_train = df_train[df_train['system_id']==alg]\n",
    "    \n",
    "    unet_input = []\n",
    "    \n",
    "    for index, row in tqdm.tqdm(alg_df_train.iterrows(), total=alg_df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        mag = (mag - np.min(mag)) / (np.max(mag) - np.min(mag))\n",
    "        # qui puoi concatenare le magnitude per l'input alla rete nella variabile unet_input\n",
    "        unet_input.append(mag)\n",
    "        \n",
    "    unet_input = np.array(unet_input)\n",
    "    unet_input = unet_input[..., np.newaxis]\n",
    "    \n",
    "    train_validation_index = int(np.round(unet_input.shape[0] * 0.75))\n",
    "    X_train = unet_input[:train_validation_index]\n",
    "    X_valid = unet_input[train_validation_index:]\n",
    "\n",
    "    model_checkpoint_name = '../features/unet_norm/models/train_nfft_{}_hop_size_{}_alg_{}.ckpt'.format(\n",
    "        nfft, hop_size, alg)\n",
    "    \n",
    "    history_name = '../features/unet_norm/history/train_nfft_{}_hop_size_{}_alg_{}.npy'.format(\n",
    "        nfft, hop_size, alg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model, encoder = unet(None, (input_size, input_size, 1))\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_checkpoint_name, \n",
    "                                 monitor='val_loss', \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=True, \n",
    "                                 mode='auto',\n",
    "                                 verbose=0)\n",
    "    \n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_valid, X_valid),\n",
    "                    callbacks=[ checkpoint, early])\n",
    "    np.save(history_name, history.history)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "\n",
    "for alg in alg_list:\n",
    "    alg_df_train = df_train[df_train['system_id']==alg]\n",
    "    \n",
    "    unet_input = []\n",
    "    \n",
    "    for index, row in tqdm.tqdm(alg_df_train.iterrows(), total=alg_df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # qui puoi concatenare le magnitude per l'input alla rete nella variabile unet_input\n",
    "        unet_input.append(mag)\n",
    "        \n",
    "    unet_input = np.array(unet_input)\n",
    "    unet_input = unet_input[..., np.newaxis]\n",
    "    \n",
    "    train_validation_index = int(np.round(unet_input.shape[0] * 0.75))\n",
    "    X_train = unet_input[:train_validation_index]\n",
    "    X_valid = unet_input[train_validation_index:]\n",
    "\n",
    "    model_checkpoint_name = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}.ckpt'.format(\n",
    "        nfft, hop_size, alg)\n",
    "    \n",
    "    history_name = '../features/unet/history/train_nfft_{}_hop_size_{}_alg_{}.npy'.format(\n",
    "        nfft, hop_size, alg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model, encoder = unet(None, (input_size, input_size, 1))\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_checkpoint_name, \n",
    "                                 monitor='val_loss', \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=True, \n",
    "                                 mode='auto',\n",
    "                                 verbose=0)\n",
    "    \n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_valid, X_valid),\n",
    "                    callbacks=[ checkpoint, early])\n",
    "    np.save(history_name, history.history)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "for alg in alg_list:\n",
    "    \n",
    "    # carichiamo il modello corrispondente dalla cartella \n",
    "    model_folder = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "\n",
    "    # creiamo un campo in un dataframe per ora vuoto\n",
    "    feat_name = 'unet_mse_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "    df_train[feat_name] = np.nan\n",
    "    \n",
    "    break\n",
    "    for index, row in tqdm.tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # calcoliamo l'MSE \n",
    "        \n",
    "        # salviamo l'MSE nel corrispondente campo\n",
    "        \n",
    "        # df_train.at[index, feat_name] = mse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev_feat_root_path = '../features/bicoherences/dev_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "for alg in alg_list:\n",
    "    \n",
    "    # carichiamo il modello corrispondente dalla cartella \n",
    "    model_folder = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "\n",
    "    # creiamo un campo in un dataframe per ora vuoto\n",
    "    feat_name = 'unet_mse_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "    df_train[feat_name] = np.nan\n",
    "    \n",
    "    break\n",
    "    for index, row in tqdm.tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # calcoliamo l'MSE \n",
    "        \n",
    "        # salviamo l'MSE nel corrispondente campo\n",
    "        \n",
    "        # df_train.at[index, feat_name] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_feat_root_path = '../features/bicoherences/train_nfft_{}_hop_size_{}'.format(nfft, hop_size)\n",
    "\n",
    "for alg in alg_list:\n",
    "    \n",
    "    # carichiamo il modello corrispondente dalla cartella \n",
    "    model_folder = '../features/unet/models/train_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "\n",
    "    # creiamo un campo in un dataframe per ora vuoto\n",
    "    feat_name = 'unet_mse_nfft_{}_hop_size_{}_alg_{}'.format(nfft, hop_size, alg)\n",
    "    df_train[feat_name] = np.nan\n",
    "    \n",
    "    break\n",
    "    for index, row in tqdm.tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "        feat_path = os.path.join(train_feat_root_path, row['audio_filename'] + '.npy')\n",
    "        bicoh = np.load(feat_path)\n",
    "        mag = np.abs(bicoh)\n",
    "        phase = np.angle(bicoh)\n",
    "        \n",
    "        # calcoliamo l'MSE \n",
    "        \n",
    "        # salviamo l'MSE nel corrispondente campo\n",
    "        \n",
    "        # df_train.at[index, feat_name] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
